return {
  "supermaven-inc/supermaven-nvim",
  config = function()
    require("supermaven-nvim").setup({
      keymaps = {
        accept_suggestion = "<C-a>",
      },
    })
  end,
}


-- return {
--   {
--     "milanglacier/minuet-ai.nvim",
--     config = function()
--       require("minuet").setup({
--         -- Your configuration options here
--         cmp = {
--           enable_auto_complete = true,
--         },
--         blink = {
--           enable_auto_complete = true,
--         },
--         virtualtext = {
--           auto_trigger_ft = {},
--           keymap = {
--             -- accept whole completion
--             accept = "<C-A>",
--             -- accept one line
--             accept_line = "<C-a>",
--             -- accept n lines (prompts for number)
--             -- e.g. "A-z 2 CR" will accept 2 lines
--             accept_n_lines = "<C-z>",
--             -- Cycle to prev completion item, or manually invoke completion
--             prev = "<C-[>",
--             -- Cycle to next completion item, or manually invoke completion
--             next = "<C-]>",
--             dismiss = "<C-e>",
--           },
--
--           show_on_completion_menu = false,
--         },
--         -- the maximum total characters of the context before and after the cursor
--         -- 16000 characters typically equate to approximately 4,000 tokens for
--         -- LLMs.
--         context_window = 16000,
--         -- when the total characters exceed the context window, the ratio of
--         -- context before cursor and after cursor, the larger the ratio the more
--         -- context before cursor will be used. This option should be between 0 and
--         -- 1, context_ratio = 0.75 means the ratio will be 3:1.
--         context_ratio = 0.75,
--         throttle = 1000, -- only send the request every x milliseconds, use 0 to disable throttle.
--         -- debounce the request in x milliseconds, set to 0 to disable debounce
--         debounce = 400,
--         -- Control notification display for request status
--         -- Notification options:
--         -- false: Disable all notifications (use boolean false, not string "false")
--         -- "debug": Display all notifications (comprehensive debugging)
--         -- "verbose": Display most notifications
--         -- "warn": Display warnings and errors only
--         -- "error": Display errors only
--         notify = "warn",
--         -- The request timeout, measured in seconds. When streaming is enabled
--         -- (stream = true), setting a shorter request_timeout allows for faster
--         -- retrieval of completion items, albeit potentially incomplete.
--         -- Conversely, with streaming disabled (stream = false), a timeout
--         -- occurring before the LLM returns results will yield no completion items.
--         request_timeout = 3,
--         -- If completion item has multiple lines, create another completion item
--         -- only containing its first line. This option only has impact for cmp and
--         -- blink. For virtualtext, no single line entry will be added.
--         add_single_line_entry = true,
--         -- The number of completion items encoded as part of the prompt for the
--         -- chat LLM. For FIM model, this is the number of requests to send. It's
--         -- important to note that when 'add_single_line_entry' is set to true, the
--         -- actual number of returned items may exceed this value. Additionally, the
--         -- LLM cannot guarantee the exact number of completion items specified, as
--         -- this parameter serves only as a prompt guideline.
--         n_completions = 3,
--         -- Defines the length of non-whitespace context after the cursor used to
--         -- filter completion text. Set to 0 to disable filtering.
--         --
--         -- Example: With after_cursor_filter_length = 3 and context:
--         --
--         -- "def fib(n):\n|\n\nfib(5)" (where | represents cursor position),
--         --
--         -- if the completion text contains "fib", then "fib" and subsequent text
--         -- will be removed. This setting filters repeated text generated by the
--         -- LLM. A large value (e.g., 15) is recommended to avoid false positives.
--         after_cursor_filter_length = 15,
--         -- proxy port to use
--         proxy = nil,
--
--         provider = "codestral",
--         provider_options = {
--           -- see the documentation in each provider in the following part.
--           provider_options = {
--             gemini = {
--               model = "gemini-2.0-flash",
--               system = "see [Prompt] section for the default value",
--               few_shots = "see [Prompt] section for the default value",
--               chat_input = "See [Prompt Section for default value]",
--               stream = true,
--               api_key = "GEMINI_API_KEY",
--               optional = {},
--             },
--             -- openai = {
--             --         model = 'gpt-4o-mini',
--             --         system = "see [Prompt] section for the default value",
--             --         few_shots = "see [Prompt] section for the default value",
--             --         chat_input = "See [Prompt Section for default value]",
--             --         stream = true,
--             --         api_key = 'OPENAI_API_KEY',
--             --         optional = {
--             --             -- pass any additional parameters you want to send to OpenAI request,
--             --             -- e.g.
--             --             -- stop = { 'end' },
--             --             -- max_tokens = 256,
--             --             -- top_p = 0.9,
--             --         },
--             --     },
--             -- openai_compatible = {
--             -- good
--             -- api_key = "FIREWORKS_API_KEY", -- will read the environment variable FIREWORKS_API_KEY
--             -- good
--             -- api_key = function()
--             -- return "sk-xxxx"
--
--             -- bad
--             -- api_key = "sk-xxxx",
--             -- }
--           },
--         },
--         -- see the documentation in the `Prompt` section
--         default_template = {
--           template = "...",
--           prompt = "...",
--           guidelines = "...",
--           n_completion_template = "...",
--         },
--         default_fim_template = {
--           prompt = "...",
--           suffix = "...",
--         },
--         default_few_shots = { "..." },
--         default_chat_input = { "..." },
--         -- Config options for `Minuet change_preset` command
--         presets = {},
--       })
--     end,
--   },
--   { "nvim-lua/plenary.nvim" },
--   -- optional, if you are using virtual-text frontend, nvim-cmp is not
--   -- required.
--   { "hrsh7th/nvim-cmp" },
--   -- optional, if you are using virtual-text frontend, blink is not required.
--   { "Saghen/blink.cmp" },
-- }
